---
title: "Homework6 - Group 6"
author: "Shahzad Anasari, Jordan Daugherty and Karen Ochie"
date: "10/18/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(dplyr)
library(ibd)
library(tidyverse)
library(reshape2)
library(datasets)
library(devtools)
library(scatterplot3d)
library(rgl)
library(ggplot2)
library(ggbiplot)
library(caret)
library(MASS)
library(HSAUR2)
library(outliers)
library(plyr)
library(readr)
library(car)
library(klaR)
library(caTools)
library(tidyverse)
library(pls)
library(AppliedPredictiveModeling)
library(visdat)
library(DataExplorer)
library(gridExtra)
library(grid)
library(ggcorrplot)
library(mice)
library(FactoMineR)
library(factoextra)
library(Hmisc)
library(fastDummies)
library(outliers)

training_data = data.frame(read.csv(file = "Train.csv", stringsAsFactors = TRUE)) %>% mutate_all(na_if,"")
testing_data = data.frame(read.csv(file = "Test.csv", stringsAsFactors = TRUE)) %>% mutate_all(na_if,"")
```

**Learning Objective:** Data wrangling, regression modeling and analysis

**Problem 1 - Online Retail Sales Prediction**

*(a) Data Preparation and Modeling*

i. Data Understanding
```{r, fig.width=14, fig.height=8, include=TRUE}
ggplot(data=training_data, aes(y=revenue, x=operatingSystem))+
  geom_point(color="blue")+
  xlab("Operating Sysem")+
  ylab("Revenue")+
  labs(title="Figure 1: Revenue vs Operating System")+
  dark_theme_bw()

```
Figure 1 illustrates the relationship between different operating systems used and the amount of revenue gained from each operating system. We can take away some good information from this graph that can help us predict future revenue based on what operating system was used.
``` {r, fig.width=14, include=TRUE}
ggplot(data=training_data, aes(y=revenue, x=subContinent))+
  geom_point(aes(color=channelGrouping))+
  labs(x="Sub-Continent",y="Customer Revenue",title="Figure 2: Revenue vs. Sub-Continent",size=10)+
  scale_colour_discrete("Channel\nGrouping")+
  dark_theme_light()+
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))
```
Figure 2 gives us information from a different angle, showing us the amount of revenue gained based on the continent the user is from. Channel grouping was also added to show what category was used by the user.

Overall, we can learn a lot about what different factors have high influences on the amount of revenue gained by a customer. From Figure 1, we can see that a majority of revenue is gained when the operating systems Macintosh, Chrome, Windows and Android are used. Figure 2 illustrates that Northern America is by far the sub-continent with the most revenue gained. The channel grouping also provides us with more incite on which channel groupings are used more frequently for purchases. We can also see that there variables that contain many factor levels that dont typically produce any revenue. 

ii. Data Preparation
```{r, include=TRUE, fig.height=16}
#Finding percentage of missing data in each variable
percentagesDf = data.frame(colnames(training_data))
for(i in 1:ncol(training_data)) {       # for-loop over columns
  percentagesDf[i,2] = (round((sum(is.na(training_data[,i])))
                              /dim(training_data)[1],5))*100
}
names(percentagesDf)[1] = "Column_Name"
names(percentagesDf)[2] = "Missing_Percentage"
percentagesDf = as.data.frame(percentagesDf)

ggplot(data = percentagesDf, mapping = aes(x = reorder(Column_Name, -Missing_Percentage), Missing_Percentage)) + 
  geom_bar(stat = "identity",aes(fill=Missing_Percentage), position = 'dodge') + 
  coord_flip()+
  geom_label(aes(label = Missing_Percentage,fill=Missing_Percentage),color = "Red")+
  xlab("Column Name")+
  ylab("Missing Percentage")+
  ggtitle("Training Data Missing Percentage")


percentagesDf = data.frame(colnames(testing_data))
for(i in 1:ncol(testing_data)) {       # for-loop over columns
  percentagesDf[i,2] = (round((sum(is.na(testing_data[,i])))
                              /dim(testing_data)[1],5))*100
}
names(percentagesDf)[1] = "Column_Name"
names(percentagesDf)[2] = "Missing_Percentage"
percentagesDf = as.data.frame(percentagesDf)

ggplot(data = percentagesDf, mapping = aes(x = reorder(Column_Name, -Missing_Percentage), Missing_Percentage)) + 
  geom_bar(stat = "identity",aes(fill=Missing_Percentage), position = 'dodge') + 
  coord_flip()+
  geom_label(aes(label = Missing_Percentage,fill=Missing_Percentage),color = "Red")+
  xlab("Column Name")+
  ylab("Missing Percentage")+
  ggtitle("Testing Data Missing Percentage")
```

```{r, include=FALSE}

sum(is.na(training_data$country))
str(training_data)

#Remove columns that a majority of the values are N/A (newVisits:adContent)
training_data = dplyr::select(training_data,-c(adwordsClickInfo.isVideoAd,
                                               campaign,
                                               adContent,
                                               metro,
                                               adwordsClickInfo.page,
                                               city,
                                               adwordsClickInfo.slot,
                                               adwordsClickInfo.gclId,
                                               adwordsClickInfo.adNetworkType,
                                               keyword,
                                               referralPath,
                                               region,
                                               networkDomain,
                                               topLevelDomain,
                                               bounces,
                                               sessionId,
                                               visitStartTime,
                                               date
))

testing_data = dplyr::select(testing_data,-c(adwordsClickInfo.isVideoAd,
                                             campaign,
                                             adContent,
                                             metro,
                                             adwordsClickInfo.page,
                                             city,
                                             adwordsClickInfo.slot,
                                             adwordsClickInfo.gclId,
                                             adwordsClickInfo.adNetworkType,
                                             keyword,
                                             referralPath,
                                             region,
                                             networkDomain,
                                             topLevelDomain,
                                             bounces,
                                             sessionId,
                                             visitStartTime,
                                             date
))
```
```{r, include=FALSE}
###############################################################################
#                                   IMPUTE                                    #
###############################################################################
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# imputing the missing values of medium using probability 
set.seed(1)
mediumFreq = na.omit(count(training_data,'medium'))
totalMedium = sum(mediumFreq$freq[1:5])
mediumFreq[,3] = mediumFreq[,2]/totalMedium
names(mediumFreq)[3] = "Probability"
training_data$medium[is.na(training_data$medium)] <-as.factor(sample(mediumFreq$medium[1:5], 
                                                                     size=1, replace=TRUE, prob=mediumFreq$Probability[1:5]
))
sum(is.na(training_data$medium))
#################################################################
# browser has only 1 na value, just going to use mode imputation on this
browserFreq = count(training_data,'browser')
browserFreq
training_data$browser[is.na(training_data$browser)] <- getmode(training_data$browser) 
# impute operating System via probability
operatingSystemFreq = na.omit(count(training_data,'operatingSystem'))
totaloperatingSystem = sum(operatingSystemFreq$freq[1:15])
operatingSystemFreq[,3] = operatingSystemFreq[,2]/totaloperatingSystem
names(operatingSystemFreq)[3] = "Probability"
training_data$operatingSystem[is.na(training_data$operatingSystem)] <-as.factor(sample(operatingSystemFreq$operatingSystem[1:15], 
                                                                                       size=1, replace=TRUE, prob=operatingSystemFreq$Probability[1:15]
))
sum(is.na(training_data$operatingSystem))
################################################
# No way to know if its not or is a new visit as every non NA value is a 1. Cannot compute a probability so i will assume
# that if it is NA that it is not a new visit. 
training_data$newVisits[is.na(training_data$newVisits)] = 0
# imputing page views with rounded down average
training_data$pageviews[is.na(training_data$pageviews)] = round(mean(!is.na(training_data$pageviews)))
##############################################
testing_data$newVisits[is.na(testing_data$newVisits)] = 0
# imputing page views with rounded down average
testing_data$pageviews[is.na(testing_data$pageviews)] = round(mean(!is.na(testing_data$pageviews)))
###############################################
# Check if there is a empty country or region or subContinent are all fields empty?
emptyLocation <- training_data[is.na(training_data$country),] 
emptyLocation

levels(training_data$country) = c(levels(training_data$country),"Unknown")
levels(training_data$continent) = c(levels(training_data$continent),"Unknown")
levels(training_data$subContinent) = c(levels(training_data$subContinent),"Unknown")

training_data$country[is.na(training_data$country)] = as.factor("Unknown")
training_data$continent[is.na(training_data$continent)] = as.factor("Unknown")
training_data$subContinent[is.na(training_data$subContinent)] = as.factor("Unknown")

# Since source has only 2 missing values i will just use mode imputation 
sourceFreq = count(training_data,'source')
sourceFreq
training_data$source[is.na(training_data$source)] <- getmode(training_data$source)

# show NA for all columns 

percentagesDf = data.frame(colnames(training_data))

for(i in 1:ncol(training_data)) {       # for-loop over columns
  percentagesDf[i,2] = sum(is.na(training_data[,i]))
  
}
names(percentagesDf)[1] = "Column name"
names(percentagesDf)[2] = "Missing Count"

percentagesDf

###############################################################################
#                                   END IMPUTE                                #
###############################################################################
```

```{r, include=FALSE}
###############################################################################
#                            OUTLIER RESOLUTION                               # 
###############################################################################
grubbs.test(training_data$revenue)

outliers = training_data[training_data$revenue >= 15980.79,] 

# just one person is an outlier in revenue. 
dim(outliers)

training_data = training_data[training_data$revenue < 15980.79,]

###############################################################################
#                       END OF OUTLIER RESOLUTION                             # 
###############################################################################
```

```{r, include=FALSE}
###############################################################################
#                             AGGREGATION                                     #
###############################################################################
#Convert all character values and numeric values to ints and factors
training_data[sapply(training_data, is.numeric)] <- lapply(training_data[sapply(training_data, is.numeric)],as.integer)
testing_data[sapply(testing_data, is.numeric)] <- lapply(testing_data[sapply(testing_data, is.numeric)],as.integer)

# Confirm the changes have taken place 
str(training_data)


t1 = training_data %>%  
  mutate(subContinent = fct_lump(as.factor(subContinent),n=5)) %>%
  mutate(browser = fct_lump(as.factor(browser),n=8)) %>%
  mutate(country = fct_lump(as.factor(country),n=8))%>%
  mutate(operatingSystem = fct_lump(as.factor(operatingSystem),n=6)) %>% 
  group_by(custId) %>%
  dplyr::summarise(n = n(),
                   totalRevenue = sum(revenue),
                   pageviews = mean(pageviews, na.rm=TRUE),
                   total_visit= max(visitNumber),
                   visitGap = mean(timeSinceLastVisit)/3600
                   ,subContinent = getmode(subContinent)
                   ,browser = getmode(browser)
                   ,operatingSystem = getmode(operatingSystem)
                   ,country = getmode(country)
                   
                   
                   
                   
  )

t2 = testing_data %>%  
  mutate(subContinent = fct_lump(as.factor(subContinent),n=5)) %>%
  mutate(browser = fct_lump(as.factor(browser),n=8)) %>%
  mutate(country = fct_lump(as.factor(country),n=8))%>%
  mutate(operatingSystem = fct_lump(as.factor(operatingSystem),n=6)) %>% 
  group_by(custId) %>%
  dplyr::summarise(n = n(),
                   pageviews = mean(pageviews, na.rm=TRUE),
                   total_visit= max(visitNumber),
                   visitGap = mean(timeSinceLastVisit)/3600
                   ,subContinent = getmode(subContinent)
                   ,browser = getmode(browser)
                   ,operatingSystem = getmode(operatingSystem)
                   ,country = getmode(country)
                   
                   
  )
t2$subContinent[is.na(t2$subContinent)] = as.factor("Other")
t2$browser[is.na(t2$browser)] = as.factor("Other")
t2$operatingSystem[is.na(t2$operatingSystem)] = as.factor("Other")

table(t2$subContinent)
table(t1$subContinent)

###############################################################################
#                           END AGGREGATION                                  #
###############################################################################
```

```{r, include=FALSE}

###############################################################################
#                                   OLS MODEL                                 #
###############################################################################

tempFit <- lm(data=t1, log(totalRevenue + 1) ~ .-custId)
summary(tempFit)


sqrt(mean((tempFit$fitted.values-log(t1$totalRevenue+1))^2))


out1 = data.frame(t2$custId,predict(tempFit,newdata=t2))
write.csv(out1,"out2.csv", row.names = FALSE)

sum(is.na(out1))

fitControl <- trainControl(method="repeatedcv",number=20, repeats = 5)
lassoGrid <- expand.grid(fraction=seq(0.1,0.99,length=100))
fit_ols <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
                 method="lm",
                 trControl = fitControl,
                 na.action = na.pass)
fit_ols

pred<-predict(fit_ols,newdata=t2,na.action = na.pass)
pred
dim(pred)

submissionDf1 <- data.frame(custId=t2$custId, predRevenue=pred)
#create csv file for uploading to kaggle
write.csv(submissionDf1, "submission2.csv", row.names=FALSE)

```

```{r}

###############################################################################
#                                   PLS MODEL                                 #
###############################################################################

fitControl <- trainControl(method="repeatedcv",number=20, repeats = 5)
lassoGrid <- expand.grid(fraction=seq(0.1,0.99,length=100))
fit_pls <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
                 method="pls",
                 trControl = fitControl,
                 na.action = na.pass)
fit_pls

pred<-predict(fit_pls,newdata=t2,na.action = na.pass)
pred

model3 <- plsr(log(totalRevenue+1)~.-custId,data=t1,validation="CV",scale=TRUE,method="oscorespls")

sqrt(mean((model3$fitted.values-log(t1$totalRevenue+1))^2))

pred_pls <- predict(model3,newdata=t2)
pred_pls

control <- trainControl(method = "repeatedcv",
                        number = 20,
                        repeats = 5,
                        search = "random",
                        verboseIter = TRUE)
```

```{r}

###############################################################################
#                           ELASTIC NET MODEL                                 #
###############################################################################

#TRAINING ELASTIC NET REGRESSION MODEL
elastic_model <- train(data=t1,log(totalRevenue + 1) ~ .-custId,
                       method = "glmnet",
                       na.action = na.pass,
                       preProcess = c("center", "scale"),
                       tuneLength = 10,
                       trControl = control)

elastic_model$results

pred_elastic <- predict(elastic_model,newdata=t2)


```
```



```