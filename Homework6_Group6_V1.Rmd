---
title: "Homework6 - Group 6"
author: "Shahzad Anasari, Jordan Daugherty and Karen Ochie"
date: "10/18/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(dplyr)
library(ibd)
library(tidyverse)
library(reshape2)
library(datasets)
library(devtools)
library(scatterplot3d)
library(rgl)
library(ggplot2)
library(ggbiplot)
library(caret)
library(MASS)
library(HSAUR2)
library(outliers)
library(plyr)
library(readr)
library(car)
library(klaR)
library(caTools)
library(tidyverse)
library(pls)
library(AppliedPredictiveModeling)
library(visdat)
library(DataExplorer)
library(gridExtra)
library(grid)
library(ggcorrplot)
library(mice)
library(FactoMineR)
library(factoextra)
library(Hmisc)
library(fastDummies)
library(outliers)
library(e1071)
library(hydroGOF)
library(ggdark)
library(lars)

training_data = data.frame(read.csv(file = "Train.csv", stringsAsFactors = TRUE)) %>% mutate_all(na_if,"")
testing_data = data.frame(read.csv(file = "Test.csv", stringsAsFactors = TRUE)) %>% mutate_all(na_if,"")
```

**Learning Objective:** Data wrangling, regression modeling and analysis

**Problem 1 - Online Retail Sales Prediction**

*(a) Data Preparation and Modeling*

i. Data Understanding
```{r, fig.width=14, fig.height=8, include=TRUE}
ggplot(data=training_data, aes(y=revenue, x=operatingSystem))+
  geom_point(color="blue")+
  xlab("Operating Sysem")+
  ylab("Revenue")+
  labs(title="Figure 1: Revenue vs Operating System")+
  dark_theme_bw()

```
Figure 1 illustrates the relationship between different operating systems used and the amount of revenue gained from each operating system. We can take away some good information from this graph that can help us predict future revenue based on what operating system was used.
``` {r, fig.width=14, include=TRUE}
ggplot(data=training_data, aes(y=revenue, x=subContinent))+
  geom_point(aes(color=channelGrouping))+
  labs(x="Sub-Continent",y="Customer Revenue",title="Figure 2: Revenue vs. Sub-Continent",
       size=10)+
  scale_colour_discrete("Channel\nGrouping")+
  dark_theme_light()+
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))
```
Figure 2 gives us information from a different angle, showing us the amount of revenue gained based on the continent the user is from. Channel grouping was also added to show what category was used by the user.

Overall, we can learn a lot about what different factors have high influences on the amount of revenue gained by a customer. From Figure 1, we can see that a majority of revenue is gained when the operating systems Macintosh, Chrome, Windows and Android are used. Figure 2 illustrates that Northern America is by far the sub-continent with the most revenue gained. The channel grouping also provides us with more incite on which channel groupings are used more frequently for purchases. We can also see that there variables that contain many factor levels that dont typically produce any revenue. 

ii. Data Preparation
```{r, include=TRUE, fig.height=16, fig.width=16}
#Finding percentage of missing data in each variable
percentagesDf = data.frame(colnames(training_data))
for(i in 1:ncol(training_data)) {       # for-loop over columns
  percentagesDf[i,2] = (round((sum(is.na(training_data[,i])))
                              /dim(training_data)[1],5))*100
}
names(percentagesDf)[1] = "Column_Name"
names(percentagesDf)[2] = "Missing_Percentage"
percentagesDf = as.data.frame(percentagesDf)

ggplot(data = percentagesDf, mapping = aes(x = reorder(Column_Name, -Missing_Percentage),
                                           Missing_Percentage)) + 
  geom_bar(stat = "identity",aes(fill=Missing_Percentage), position = 'dodge') + 
  coord_flip()+
  geom_label(aes(label = Missing_Percentage,fill=Missing_Percentage),color = "Red")+
  xlab("Column Name")+
  ylab("Missing Percentage")+
  ggtitle("Training Data Missing Percentage")


percentagesDf = data.frame(colnames(testing_data))
for(i in 1:ncol(testing_data)) {       # for-loop over columns
  percentagesDf[i,2] = (round((sum(is.na(testing_data[,i])))
                              /dim(testing_data)[1],5))*100
}
names(percentagesDf)[1] = "Column_Name"
names(percentagesDf)[2] = "Missing_Percentage"
percentagesDf = as.data.frame(percentagesDf)

```

*Data Preparation*

**1. Factor Selection:**
After visualizing the data set and seeing how much of each factor contains NA values, we began to select which we would keep. As a general rule any factor containing over 20
percent of the missing information we determined to be not worth imputing as any imputation methods we would use may be inaccurate or in some cases, it would be impossible to create
an accurate imputation i.e city. There would be no way for us to know which city they originated from nor what metro etc. We also removed information that we determined to be
of little value such as bounces, sessionId,visitStartTime, and date. There would be no real meaningful information gained from knowing what time a session started and modeling
confirmed that some of these factors were not of high value.
```{r, include=FALSE}

sum(is.na(training_data$country))
str(training_data)

#Remove columns that a majority of the values are N/A (newVisits:adContent)
training_data = dplyr::select(training_data,-c(adwordsClickInfo.isVideoAd,
                                               campaign,
                                               adContent,
                                               metro,
                                               adwordsClickInfo.page,
                                               city,
                                               adwordsClickInfo.slot,
                                               adwordsClickInfo.gclId,
                                               adwordsClickInfo.adNetworkType,
                                               keyword,
                                               referralPath,
                                               region,
                                               networkDomain,
                                               topLevelDomain,
                                               bounces,
                                               sessionId,
                                               visitStartTime,
                                               date
))

testing_data = dplyr::select(testing_data,-c(adwordsClickInfo.isVideoAd,
                                             campaign,
                                             adContent,
                                             metro,
                                             adwordsClickInfo.page,
                                             city,
                                             adwordsClickInfo.slot,
                                             adwordsClickInfo.gclId,
                                             adwordsClickInfo.adNetworkType,
                                             keyword,
                                             referralPath,
                                             region,
                                             networkDomain,
                                             topLevelDomain,
                                             bounces,
                                             sessionId,
                                             visitStartTime,
                                             date
))
```

**2. Imputations:**
Once we have decided on which factors are worth keeping we need to then impute the missing values in those factors. Many of the factors were categorical in nature meaning any
numerical method used to impute such as mean value imputation would be out of the question. For the categorical variables such as operating system, browser and medium the imputation
method used was a probabilistic method. Taking the occurrences of each category within the column you could create a probability distribution. When an NA is encountered randomly generate
a value using the probability distributions to select which of the existing categories will be used to impute the NA. For NewVisits and pageViews a more numerical method could
be used, since there was no way of knowing if a visit was a new visit we would just assume it wasn't a new visit. For the page views, we would determine the average page views using mean
value imputation and rounding down. Within the location data country, continent and subcontinent there were 84 values in which all 3 were unknown, since there was no possible way to know
what these three could be a new level was added called unknown and they have imputed it as such.

**3. Factor Collapsing**
Before any modeling is done we need to deal with the fact that some categories have too many levels. subContinent, country, browser, country, and operatingSystem all have many
levels and some of these levels may only have a few entries. For the Subcontinent continents and countries, we determined that over 50 percent of entries come from the united states,
using factor lump we consolidated the categorical columns into two levels each. The North American Continent, the north subcontinent, and the United States. The levels for browser
were reduced to the top 8 levels and 1 other level and the operating system column was lumped into the top 6 levels and one other level.
```{r, include=FALSE}
###############################################################################
#                                   IMPUTE                                    #
###############################################################################
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# imputing the missing values of medium using probability 
set.seed(1)
mediumFreq = na.omit(count(training_data,'medium'))
totalMedium = sum(mediumFreq$freq[1:5])
mediumFreq[,3] = mediumFreq[,2]/totalMedium
names(mediumFreq)[3] = "Probability"
training_data$medium[is.na(training_data$medium)] <-as.factor(sample(mediumFreq$medium[1:5], 
                                                                     size=1, replace=TRUE, prob=mediumFreq$Probability[1:5]
))
sum(is.na(training_data$medium))
#################################################################
# browser has only 1 na value, just going to use mode imputation on this
browserFreq = count(training_data,'browser')
browserFreq
training_data$browser[is.na(training_data$browser)] <- getmode(training_data$browser) 
# impute operating System via probability
operatingSystemFreq = na.omit(count(training_data,'operatingSystem'))
totaloperatingSystem = sum(operatingSystemFreq$freq[1:15])
operatingSystemFreq[,3] = operatingSystemFreq[,2]/totaloperatingSystem
names(operatingSystemFreq)[3] = "Probability"
training_data$operatingSystem[is.na(training_data$operatingSystem)] <-as.factor(sample(operatingSystemFreq$operatingSystem[1:15], 
                                                                                       size=1, replace=TRUE, prob=operatingSystemFreq$Probability[1:15]
))
sum(is.na(training_data$operatingSystem))
################################################
# No way to know if its not or is a new visit as every non NA value is a 1. Cannot compute a probability so i will assume
# that if it is NA that it is not a new visit. 
training_data$newVisits[is.na(training_data$newVisits)] = 0
# imputing page views with rounded down average
training_data$pageviews[is.na(training_data$pageviews)] = round(mean(!is.na(training_data$pageviews)))
##############################################
testing_data$newVisits[is.na(testing_data$newVisits)] = 0
# imputing page views with rounded down average
testing_data$pageviews[is.na(testing_data$pageviews)] = round(mean(!is.na(testing_data$pageviews)))
###############################################
# Check if there is a empty country or region or subContinent are all fields empty?
emptyLocation <- training_data[is.na(training_data$country),] 
emptyLocation

levels(training_data$country) = c(levels(training_data$country),"Unknown")
levels(training_data$continent) = c(levels(training_data$continent),"Unknown")
levels(training_data$subContinent) = c(levels(training_data$subContinent),"Unknown")

training_data$country[is.na(training_data$country)] = as.factor("Unknown")
training_data$continent[is.na(training_data$continent)] = as.factor("Unknown")
training_data$subContinent[is.na(training_data$subContinent)] = as.factor("Unknown")

# Since source has only 2 missing values i will just use mode imputation 
sourceFreq = count(training_data,'source')
sourceFreq
training_data$source[is.na(training_data$source)] <- getmode(training_data$source)

# show NA for all columns 

percentagesDf = data.frame(colnames(training_data))

for(i in 1:ncol(training_data)) {       # for-loop over columns
  percentagesDf[i,2] = sum(is.na(training_data[,i]))
  
}
names(percentagesDf)[1] = "Column name"
names(percentagesDf)[2] = "Missing Count"

percentagesDf

###############################################################################
#                                   END IMPUTE                                #
###############################################################################
```

**4. Outlier Resolution:**
After the initial data analysis, the revenue variable has a very significant outlier. This outlier was a value of 15980.79 and came from one single customer Id. This customer visited the site 6 times, but only made a purchase one time. In order to avoid the prediction model being skewed, the outlier was removed.
```{r, include=FALSE}
###############################################################################
#                            OUTLIER RESOLUTION                               # 
###############################################################################
grubbs.test(training_data$revenue)

outliers = training_data[training_data$revenue >= 15980.79,] 

# just one person is an outlier in revenue. 
dim(outliers)

training_data = training_data[training_data$revenue < 15980.79,]

###############################################################################
#                       END OF OUTLIER RESOLUTION                             # 
###############################################################################
```
**5. Data Aggregation:**
After the initial data preparation, the training and testing data was aggregated to summarize the parameter variables and modify the output variable to the natural log of the total revenue plus one for each customer Id. The summarized parameters include the average page views, the maximum number of visits, the average visit gap time and the mode for the browser, operatingSystem, country and subContinent. 
```{r, include=FALSE}
###############################################################################
#                             AGGREGATION                                     #
###############################################################################
#Convert all character values and numeric values to ints and factors
training_data[sapply(training_data, is.numeric)] <- lapply(training_data[sapply(training_data, is.numeric)],as.integer)
testing_data[sapply(testing_data, is.numeric)] <- lapply(testing_data[sapply(testing_data, is.numeric)],as.integer)

# Confirm the changes have taken place 
str(training_data)


t1 = training_data %>%  
  mutate(subContinent = fct_lump(as.factor(subContinent),n=5)) %>%
  mutate(browser = fct_lump(as.factor(browser),n=8)) %>%
  mutate(country = fct_lump(as.factor(country),n=8))%>%
  mutate(operatingSystem = fct_lump(as.factor(operatingSystem),n=6)) %>% 
  group_by(custId) %>%
  dplyr::summarise(n = n(),
                   totalRevenue = sum(revenue),
                   pageviews = mean(pageviews, na.rm=TRUE),
                   total_visit= max(visitNumber),
                   visitGap = mean(timeSinceLastVisit)/3600
                   ,subContinent = getmode(subContinent)
                   ,browser = getmode(browser)
                   ,operatingSystem = getmode(operatingSystem)
                   ,country = getmode(country)
  )

t2 = testing_data %>%  
  mutate(subContinent = fct_lump(as.factor(subContinent),n=5)) %>%
  mutate(browser = fct_lump(as.factor(browser),n=8)) %>%
  mutate(country = fct_lump(as.factor(country),n=8))%>%
  mutate(operatingSystem = fct_lump(as.factor(operatingSystem),n=6)) %>% 
  group_by(custId) %>%
  dplyr::summarise(n = n(),
                   pageviews = mean(pageviews, na.rm=TRUE),
                   total_visit= max(visitNumber),
                   visitGap = mean(timeSinceLastVisit)/3600
                   ,subContinent = getmode(subContinent)
                   ,browser = getmode(browser)
                   ,operatingSystem = getmode(operatingSystem)
                   ,country = getmode(country)
  )
t2$subContinent[is.na(t2$subContinent)] = as.factor("Other")
t2$browser[is.na(t2$browser)] = as.factor("Other")
t2$operatingSystem[is.na(t2$operatingSystem)] = as.factor("Other")
t2$country[is.na(t2$country)] = as.factor("Other")

table(t2$subContinent)
table(t1$subContinent)

###############################################################################
#                           END AGGREGATION                                  #
###############################################################################
```
*OLS Model*

Initially we tried the linear model from the stats package as a first attempt. This did not produce a very good RMSE value. The caret package train function was then tested with a K-fold cross validation re-sampling approach was also implemented. For the K-fold CV a fold value of 20 and a k value of 5 was used. This produced a better RMSE value. 

```{r, include=FALSE}


###############################################################################
#                                   OLS MODEL                                 #
###############################################################################

tempFit <- lm(data=t1, log(totalRevenue + 1) ~ .-custId)
summary(tempFit)

model2 <- step(tempFit,direction="backward")
summary(model2)

sqrt(mean((tempFit$fitted.values-log(t1$totalRevenue+1))^2))


out1 = data.frame(t2$custId,predict(tempFit,newdata=t2))
write.csv(out1,"out2.csv", row.names = FALSE)

sum(is.na(out1))

fitControl <- trainControl(method="repeatedcv",number=20, repeats = 5)

fit_ols <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
                 method="lm",
                 trControl = fitControl,
                 na.action = na.pass)
fit_ols
OLS_RMSE <- fit_ols$results$RMSE
OLS_R2 <- fit_ols$results$Rsquared

pred<-predict(fit_ols,newdata=t2,na.action = na.pass)
pred
dim(pred)

submissionDf1 <- data.frame(custId=t2$custId, predRevenue=pred)
#create csv file for uploading to kaggle
#write.csv(submissionDf1, "submission2.csv", row.names=FALSE)

```
*PLS Model*

A PLS model was then developed from the caret package using the train function with the pls method. This model also used K-fold cross validation re-sampling approach with the same fold=20 and k=5 from the OLS train model and produced an RMSE that was worse than our OLS model. From there, the pls package was tested to compare similar models from different packages would perform. The oscorespls method was used for this model, with a ncomp of 30 and a cross validation approach. This model produced a similar value to the OLS model.

```{r, include=FALSE}

###############################################################################
#                                   PLS MODEL                                 #
###############################################################################

fitControl <- trainControl(method="repeatedcv",number=20, repeats = 5)
fit_pls <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
                 method="pls",
                 trControl = fitControl,
                 na.action = na.pass)
fit_pls

pred<-predict(fit_pls,newdata=t2,na.action = na.pass)

PLS_model <- plsr(log(totalRevenue+1)~.-custId,30,data=t1,validation="CV",scale=TRUE,method="oscorespls")

PLS_RMSE<-sqrt(mean((PLS_model$fitted.values-log(t1$totalRevenue+1))^2))
PLS_RMSE

PLS_R2<-mean(R2(PLS_model,"CV",intercept = FALSE)$val[,,])
PLS_R2

pred_pls <- predict(PLS_model,newdata=t2)
pred_pls

```
 *Elastic Net Model*
 
 Here, two different caret package train methods were tested. The first a glmnet method with K-fold cross validation with 5 folds and k=5. This produced an RMSE of 0.937, which wasn't better than our OLS or our PLS models. Next, the enet method was tested on the Elastic Net model and along with a K-fold cross validation with 5 folds and a k-value of 10. This produced a significantly worse RMSE value of 1.02 with a lambda of 0.5 and a fraction of 0.3. 

```{r, indluce=FALSE, results='hide'}

###############################################################################
#                           ELASTIC NET MODEL                                 #
###############################################################################

#TRAINING ELASTIC NET REGRESSION MODEL

control <- trainControl(method = "repeatedcv",
                        number = 5,
                        repeats = 5,
                        search = "random",
                        verboseIter = TRUE)

elastic_model <- train(data=t1,log(totalRevenue + 1) ~ .-custId,
                       method = "glmnet",
                       na.action = na.pass,
                       preProcess = c("center", "scale"),
                       tuneLength = 10,
                       trControl = control)

grid <- expand.grid(
  lambda   = seq(0.5, 0.7, by=0.1),
  fraction = seq(0, 1, by=0.1)
)

ctrl <- trainControl(
  method     = 'repeatedcv',
  number     = 5,  #folds
  repeats    = 10, #repeats
  classProbs = FALSE
)


enetTune <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
  method = 'enet',
  metric = 'RMSE',
  tuneGrid = grid,
  trControl = ctrl
)

plot(elastic_model)



ELASTIC_RMSE<-min(elastic_model$results$RMSE)
ELASTIC_R2 <- elastic_model$results$Rsquared[2]


pred_elastic <- predict(elastic_model,newdata=t2)
```
```{r, include=FALSE}
elastic_model$bestTune

```
*LASSO Model*

Next, a LASSO model was tested with the caret package train function. Here a K-fold cross validation was used with 20 fold and a k-value of 5. A tune grid was also used to test 100 different fraction values. This model produced our an RMSE value better than the 3 previous model types tested. 
```{r, include=FALSE}
###############################################################################
#                               LASSO MODEL                                   #
###############################################################################
lassoGrid <- expand.grid(fraction=seq(0.1,0.99,length=100))
fit_lasso <- train(data=t1, log(totalRevenue + 1) ~ .-custId,
                   method="lasso",
                   trControl = fitControl,
                   tuneGrid = lassoGrid,
                   na.action = na.pass)

fit_lasso

LASSO_RMSE <- min(fit_lasso$results$RMSE)
LASSO_R2 <- max(fit_lasso$results$Rsquared)
```
*Ridge Regression*

Another model was then tested using the caret package train function with the glment method, a tune grid where multiple lambda values were tested, and K-fold cross validation was used with 5 folds and a k-value of 5. This model produced a RMSE that was better than the elastic net, but not better than any other model tested. 
```{r, include=FALSE}
###############################################################################
#                        RIDGE REGRESSION MODEL                              #
###############################################################################

custom = trainControl(method = "repeatedcv", number = 5, verboseIter = T, repeats = 5)

set.seed(1234)

ridge <- train(data=t1, log(totalRevenue + 1) ~ .-custId,method ='glmnet',
               tuneGrid = expand.grid(alpha = 0,lambda = seq(.0000,1,length=5 )),
               trControl = custom)
plot(ridge)
print(ridge)
plot(ridge$finalModel,xvar="lambda",label = T)
plot(varImp(ridge,scale = T))

RIDGE_RMSE <- min(ridge$results$RMSE)
RIDGE_R2 <- max(ridge$results$Rsquared)

```
*SVM Model*

Lastly, an SVM model was tested using the e1071 package and the eps-regression model type. This model produced a significant improvement to the RMSE with an RMSE of 0.74. 

```{r, include=FALSE}
###############################################################################
#                               SVM MODEL                                     #
###############################################################################
SVM_model=svm(data=t1,log(totalRevenue + 1) ~ .-custId, na.action=na.pass,type="eps-regression")


#Find RMSE
SVM_RMSE <- sqrt(mean((SVM_model$fitted-log(t1$totalRevenue+1))^2))

#Find R2
pred_svm = predict(SVM_model,newdata=t1,na.action=na.pass)
SVM_R2<-cor(pred_svm,log(t1$totalRevenue+1))^2

#submissionDf3 <- data.frame(custId=t2$custId, predRevenue=pred_svm)
#create csv file for uploading to kaggle
#write.csv(submissionDf3, "submission3.csv", row.names=FALSE)


```
*Conclusion*

Our modeling approach was to test a few variations of each model and from there look for models that 
produced the best RMSE from our cleaned data. A lot of the models that were tested, were tested with trial
and error to see how different hyperparameters would impact the RMSE values. Multiple grid techniques were used
to test multiple hyperparameter values to achieve an optimal result. One of the main problems that occurred 
during out model testing was the issue of run time with the SVM model. SVM produced the best result, but also
took the longest time to compute, which made it very time consuming to trial different variations of this model.
```{r, include=FALSE}

summary_data <- data.frame(Model=c("OLS","PLS","LASSO","elasticNet","RIDGE","SVM"),
                           Method=c("lm","plsr","lasso","glmnet","glmnet","svm"),
                           Package=c("stats","pls","caret","caret","caret","e1071"),
                           Hyperparameter=c("NA","ncomp","fraction","alpha,lambda",
                                            "alpha, lambda","cost, gamma,epsilon"),
                           Value=c("NA",31,0.5405051,"0.9012876,0.004112218",
                                   "0,0","1,0.03125,0.1"),
                           CV_RMSE=c(OLS_RMSE,PLS_RMSE,LASSO_RMSE,ELASTIC_RMSE,
                                     RIDGE_RMSE,SVM_RMSE),
                           CV_R2=c(OLS_R2,PLS_R2,LASSO_R2,ELASTIC_R2,RIDGE_R2,SVM_R2))
```
```{r}
summary_data

```